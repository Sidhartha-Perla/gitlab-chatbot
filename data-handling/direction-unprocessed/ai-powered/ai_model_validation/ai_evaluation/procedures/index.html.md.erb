---
layout: markdown_page
title: "AI Evaluation Procedures"
description: "Procedures GitLab uses to evaluate models"
---

## On this page
{:.no_toc}

- TOC
{:toc}

### Overview

The AI Validation Team’s Centralized Evaluation Framework supports the entire end-to-end process of AI feature creation, from selecting the appropriate model for a use case to evaluating the AI features’ output. AI Validation works in concert with other types of evaluation, such as SET Quality testing and diagnostic testing, but is specifically focused on the interaction with Generative AI.

The foundation of the Centralized Evaluation Framework is based upon three main elements: a prompt library, an established ground truth, and validation metrics.

1. The prompt library is composed of diverse benchmark datasets tailored to serve as a proxy to various use cases for testing at scale. These can be generated by human turk or synthetically created using heuristics relevant to the use case. Examples of use cases include code completion, code generation, and natural language questions. 
2. Depending on the use case, the method for establishing ground truth may vary. Prompt library datasets may be used as established ground truth if they contain responses to input that are both historical and limited in scope. For example, historical developer output code was used to establish ground truth for the code completion use case. In cases where ground truth is more subjective or mutable, we may use a benchmark LLM’s output to establish ground truth. Use of [foundational models ](https://about.gitlab.com/direction/ai-powered/ai_model_validation/ai_evaluation/foundation_models/) provide a baseline for understanding the performance of GitLab AI features against industry standards. 
3. Validation metrics provide a basis upon which to assess the accuracy and usefulness of Generative AI outputs against ground truth. The Centralized Evaluation Framework incorporates various validation metrics, to include but not limited to similarity score, cross similarity score, and LLM evaluator scores such as LLM consensus filtering and LLM judges. The combined output of use-case specific metrics serve as a proxy for production and mimic human judgment in accepting or rejecting AI-generated content.

### Validation Team Procedures

This guide covers the following activities for the Model Validation Process:

1. Use Case Identification
   * The Validation Team coordinates with Product Teams to identify distinct use cases for AI-Powered features. These start with customer problems and pain points.
2. Model Selection
   * The Validation Team identifies new industry models for identified use case testing, these could be proprietary or open source software (OSS). We evaluate models with many heuristics including: capabilities, target use cases, terms, privacy policies, as well as industry benchmarks.
3. Dataset Curation Within the Prompt Library
   * The Validation Team identifies, curates, and cleans appropriate datasets for testing use cases.
4. Establishment of Ground Truth
   * The Validation Team, based on the nature of the use case, identifies the appropriate ground truth for testing purposes.
5. Validation Processing
   * The Validation Team leverages the Prompt Library to test Product Team features, including base models and any tooling, against [validation metrics](https://about.gitlab.com/direction/ai-powered/ai_model_validation/ai_evaluation/metrics/).
   * The Validation Team works with Product Teams to craft a schema for validation results that is both useful and concise.
   * The Validation Team intends to test all production models and previously tested industry benchmark models on a regular basis, to identify model drift.
     * Support for weekly performance metrics are populated on feature dashboards,
     * Should the performance of any production model or feature performance drop, the AI Validation team will work with Product Teams to assess causes and propose remediation.
     * In the event that a model must be pulled from production, either due to quality or because third-party AI vendors have modified their terms or practices to be inconsistent with GitLab data privacy standard, the Centralized Evaluation Framework will provide a basis for alternative model selection.
6. Analysis
   * The Validation Team prepares a dashboard for Product Teams to easily access and leverage validation results.
   * The Validation Team works with Product Teams to pull meaningful insights from the validation results.

<p align="center">
    <i><br>
    Last Reviewed: 2024-10-05<br>
    Last Updated: 2024-10-05
    </i>
</p>