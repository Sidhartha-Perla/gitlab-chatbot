---
layout: markdown_page
title: "Group Direction - AI Model Validation"
description: "Research and Evaluate AI/ML models to enhance the GitLab product with Generative AI."
canonical_path: "/direction/ai-powered/ai_model_validation/"
---

## On this page
{:.no_toc}

- TOC
{:toc}

---

## Overview

**Enabling Excellence in AI Model Evaluation: Realizing the Potential of Generative AI Models**

Our vision is to establish a comprehensive framework that enables users to evaluate and unlock the full potential of Generative AI (GenAI) models, including those from Google, Anthropic, the OSS community, and beyond. By providing a unified platform with advanced evaluation tools, actionable insights, and a focus on ethical considerations, we aim to foster trust, transparency, and continual improvement in AI technologies.

---

### Categories 

This group consists of the following categories:

- [AI Research](/direction/ai-powered/ai_model_validation/ai_research): Identifies and explores AI/ML models to support use cases across GitLab's stages and groups, enhancing the DevSecOps experience for users.  
- [AI Evaluation](/direction/ai-powered/ai_model_validation/ai_evaluation): Builds automated and scalable tools to evaluate model performance, output quality, and accuracy against specific inputs.

---

## Focus

The AI Model Validation group began in 2023 by laying a foundation to evaluate large language models (LLMs) for AI-powered GitLab features. Initially focused on testing Code Suggestions, the scope quickly expanded to include multi-turn conversations (Chat) and specialized evaluations, such as vulnerability data.

In 2024, we prioritized building a robust evaluation system to support GitLab Duo. The **Central Evaluation Framework (CEF)** enables large-scale evaluations, informing decisions about AI model selection. Learn more about how we [govern AI model decisions](#) and take a [deep dive into the CEF](/blog/2024/05/09/developing-gitlab-duo-how-we-validate-and-test-ai-models-at-scale/).

---

## Strategic Overview

Our strategy addresses three key questions: **Why**, **When**, and **How Much**.

- **Why**: The goal of the Model Validation team is to ensure AI models used within GitLab are thoroughly validated and reliable. By consolidating tools like ELI5 and CEF, optimizing datasets, and streamlining evaluation processes, we consistently deliver high-quality AI features.
- **When**: The FY26 strategy is broken into quarterly objectives:
  - **Q1**: Establishing foundational tools and datasets.
  - **Q2**: Advancing tools, datasets, and evaluators.
  - **Q3**: Scaling evaluations and introducing context management.
  - **Q4**: Finalizing metrics and preparing for future scalability.
- **How Much**: Resources are allocated to consolidate tools, manage datasets, evaluate foundational models, and monitor performance metrics.

---

## Vision

The Model Validation team ensures reliable AI models by optimizing datasets, unifying evaluation tools, and enhancing infrastructure. We refine processes, tools, and frameworks to maintain trust, performance, and scalability in GitLab's AI features.

---

## FY26 Roadmap: Key Goals

### Q1: Foundation Building
- **Tool Consolidation**: Develop a blueprint for unifying tools like ELI5, Langsmith, and CEF.
- **Datasets**: Create guidelines and a single source of truth (SSoT) for dataset management.

### Q2: Expansion & Quality Focus
- **Evaluators**: Build pre-configured LLM judges and validation processes.
- **Datasets**: Automate data-fetching logic for seamless integration.

### Q3: Scaling & Context Management
- **Foundational Models**: Automate model functionality for easier updates.
- **Context**: Introduce robust context management for evaluations.

### Q4: Metrics & Future-Proofing
- **Performance Metrics**: Define and implement latency and token usage tracking.
- **Infrastructure**: Review and scale validation systems for long-term demands.

---

*Last Reviewed: 2024-12-11  
Last Updated: 2024-12-11*
</p>